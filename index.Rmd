---
title       : Greenplum Overview
subtitle    : 
author      : Rupen Bandyopadhyay
job         : EMC$^2$
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

## Greenplum Database is now open source
  
<br/>
<br/>
  
1. Visit [http://greenplum.org](http://greenplum.org). There is a link to get a sandbox VM with pretty much everything installed.  

2. View the code repository at [https://github.com/greenplum-db/gpdb](https://github.com/greenplum-db/gpdb)  

3. Get the documentation from [http://gpdb.docs.pivotal.io](http://gpdb.docs.pivotal.io)  

4. Subscribe to the mailing lists if you want to remain informed about all recent changes or proposed changes.  

5. Contribute!  


--- .class #id 

## Greenplum as MPP

Everything is distributed to multiple servers. Scalability is linear to number of servers.    

  1. Data  
    i. More data storage (Big Data)  
    ii. Faster read and write performance compared to a centralized storage system ike SAN.
  
  2. Processing  
    i. Distributed Data Load   
    ii. Distributed query Processing  
    iii. Even within a single server, data is processed in parallel to acheieve faster processing  
    iv. Query Processor ensures that bulk of the processing happen independently in servers
  
  3. Database Management  
    i. Parallel / Distributed backup and restore  
    ii. Parallel / Distributed Table analysis and space management  
    iii. Pararrel / Distributed server administration through gpssh and gpscp  

--- 

## Greenplum Server Architecture

1. 2 Master Servers, N segment servers all connected through private high speed network (10GBPS)  
2. The master server is the interface to the external world. A user connects to the master and submits queries (SQLs) to the master. The standby master server acts as a failover server for the master.  
3. The master server runs a single PostgreSQL database.
4. Each of the N segment servers run 2M segment databases, M primary and M mirror databases. Each primary segment database (NM of them) has a corresponding mirror database that resides on a different server. The mirroring happens in real time so they are always synchronized. If / When a segment server fails, the corresponding mirror databases take over so the system runs.  
5. For the DCA V1 Greenplum appliances running at IRS, N = 16, M = 6. For the new DCA V2s, M = 8.  
6. The Greenplum Database Architecture is explained in detail in [Official Greenplum Documentation](http://gpdb.docs.pivotal.io/4360/admin_guide/intro/arch_overview.html)  

--- 

## Table Design Fundamentals (Distribution Policy)

Data for all tables in Greenplum are distributed to the segment databases. The master server simply has the table structure and statistical information regarding the table.  

Data is distributed using hash-bucketing. The user creating the table has control over the parameters (columns) that would be used for this hash bucketing. For efficient processing and storage, we need to make sure that the data is distributed reasonably evenly to the NM segment databases. The following strategies may be employed to find a good distribution key for a table. Multiple columns can be used for a distribution key.

1. Do not use column(s) with few distinct values. Consider the extreme case where a column has one unique value. All data will remain in one segment database.  
2. Do not use columns where a few values have a very high chance of occuring compared to others.  
3. In general, use values that are fairly unique.  
4. Avoid using too many columns as distribution key.  

---

## Table Design Fundamentals (Distribution Policy)

5. For large tables frequetly joined to each other, use the join key (or part of the join key) as the distribution key.  
6. If you have a primary key, the distribution key must be the primary key columns or a subset.
7. If you are not able to come up with any strategy, use random distribution.  
8. Check how well your data is distributed using the system generated column gp_segment_id.  

```{r eval=FALSE}
select gp_segment_id,  
       count(*)  
from   my_table  
group by gp_segment_id;  
```  

If the counts are too uneven, consider re-adjusting your distribution policy.

---

## Table Design Fundamentals (Data Types)
<br/>
Consider using the most efficient data type for your requirement. You will have smaller storage footprint (Remember BDA is a shared environment) and faster and more efficient processing.

1. Prefer smallint, int, bigint, real, double precision over the generic numeric data types.  
2. Choose the correct date/time data type: date, timestamp, timestamp with time zone.  
3. Read more about datatypes here: [http://www.postgresql.org/docs/8.2/static/datatype.html](http://www.postgresql.org/docs/8.2/static/datatype.html)
4. There is a boolean data type available for true/false values.
5. When using sequences to auto-generate Ids, use a high cache value (100+) for the sequence.  
6. Greenplum supports more complex data structures like arrays and XML as data types. Make sure you have a good plan and justification before using them.  

---  

## Table Design Fundamentals (Partitioning)

<br/>
Greenplum lets you fine tune storage, performance and archival through partitioning. This is a way to subdivide the data for a table within each segment database using user-defined buckets. Both list and range partitioning are supported along with multi-level partitions.  

Please keep the following in mind when deciding to partition a table:  

1. Because the data is already divided using distribution, further subdivision using partitions may be more inefficient. Consider a 100GB table in a 96 segment database DCA. Data is already split to 1GB segments. Applying 10 partitions on top of that will not help a lot, but will come with additional partition management tasks.  

2. For column-oriented tables, data is further subdivided by columns. Consider the table size before partitioning a column-oriented table.  

---  

## Table Design Fundamentals (Orientation and Compression)

Tables can be further optimized by adding orientation and compression.

**Orientation**:  
1. Heap => Regular tables, use for tables that are updated, deleted very often and truncate / insert is not viable.  
2. Append Optimized => Storage is optimized, but updates / deletes should be less frequent.  
3. Columnar => Append optimized be default. Stores every column separately, so when you select a few columns from a table, IO is faster.  

**Compression**:  
1. Tables can be compressed using a QUICKLZ, RLE_TYPE(columnar only) or ZLIB algorithm.
2. Compression level of 1-9 can be specified  
3. Each partition can be compressed differently.  

---  

## BDA Environment Considerations

BDA is a shared environment

You will be sharing the resources of a single Greenplum Instance. So it is important to plan, optimize and discuss viability of the following:  

1. Storage Requirement  
2. ETL processing  
   i. Time  
   ii. Frequency
   iii. Number of paralell Greenplum Connections needed  
3. Reporting  
   i. Number of concurrent users  
   ii. Average report run-times or ETAs  
   iii. Negative TIN requirements  
   
---
